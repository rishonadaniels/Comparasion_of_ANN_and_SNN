{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clpxy8GB3W-r",
        "outputId": "c297aa1c-2657-4985-ce40-81edaab0306b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/5], Loss: 0.7835, Training Accuracy: 73.15%\n",
            "Epoch [2/5], Loss: 0.6436, Training Accuracy: 77.72%\n",
            "Epoch [3/5], Loss: 0.6023, Training Accuracy: 79.13%\n",
            "Epoch [4/5], Loss: 0.5829, Training Accuracy: 79.77%\n",
            "Epoch [5/5], Loss: 0.5676, Training Accuracy: 80.43%\n",
            "Training time: 8424.16 seconds\n",
            "Accuracy on the test set: 93.45%\n",
            "Inference time: 106.41 seconds for the entire test set\n",
            "Inference time per sample: 0.010641 seconds\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1️⃣ Hyperparameters\n",
        "# -----------------------------\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001  # Lower LR for fine-tuning\n",
        "weight_decay = 1e-4  # Regularization\n",
        "momentum = 0.9  # For SGD (if used instead of Adam)\n",
        "\n",
        "# -----------------------------\n",
        "# 2️⃣ Load CIFAR-10 Dataset\n",
        "# -----------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),  # Resize to 224x224 (ViT input size)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),  # Stronger augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize test images to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 3️⃣ Load Pretrained ViT Model\n",
        "# -----------------------------\n",
        "model = models.vit_b_16(pretrained=True)\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)  # Modify last layer for CIFAR-10\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss Function & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# -----------------------------\n",
        "# 4️⃣ Fine-Tuning Function\n",
        "# -----------------------------\n",
        "training_accuracies = []\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)  # Move to GPU\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute training accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        training_accuracy = 100 * correct / total\n",
        "        training_accuracies.append(training_accuracy)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {training_accuracy:.2f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5️⃣ Evaluation Function\n",
        "# -----------------------------\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move to GPU\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
        "\n",
        "# -----------------------------\n",
        "# 6️⃣ Main Loop (Train & Test)\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "    train()\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "    # Measure inference time\n",
        "    start_time = time.time()\n",
        "    test()\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    print(f\"Inference time: {inference_time:.2f} seconds for the entire test set\")\n",
        "\n",
        "    # Calculate per-sample inference time\n",
        "    per_sample_inference_time = inference_time / len(test_dataset)\n",
        "    print(f\"Inference time per sample: {per_sample_inference_time:.6f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Save the model to a file\n",
        "model_path = \"vit_cifar10.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Measure the size of the model file\n",
        "model_size = os.path.getsize(model_path) / (1024 * 1024)  # Convert bytes to MB\n",
        "print(f\"Model size: {model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzizFuxysK_v",
        "outputId": "937d3730-2074-43c6-8806-43f85a73659b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 327.38 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8nTa9xDsTm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
